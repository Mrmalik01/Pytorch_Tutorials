{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03af7543806d644da5d7678fad71032ad9229751404418c10020b39d91c5ca178",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "source": [
    "## Vector\n",
    "\n",
    "- It is a 1D tensor corresponds to a 1D array. It is a list of tensor elements, or can be considered as a column vector\n",
    "- It is represented by small letters $x, y, z$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4]), torch.Tensor)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "a = torch.arange(5)\n",
    "a, type(a)"
   ]
  },
  {
   "source": [
    "## Matrices\n",
    "- It is a 2D tensor corresponds to a matrix. It is a 2D array of tensor elements. \n",
    "- It is represented by capital letters $X, Y, Z$\n",
    "- Mathematical notation: $A \\in R^{m*n}$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]),\n",
       " torch.Tensor,\n",
       " torch.Size([4, 5]),\n",
       " torch.Size([4, 5]),\n",
       " 4)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(4, 5)\n",
    "A, type(A), A.shape, A.size(), len(A)"
   ]
  },
  {
   "source": [
    "## Transpose of a matrix "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([4, 5]), torch.Size([5, 4]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "A_transpose = A.T\n",
    "A.shape, A_transpose.shape"
   ]
  },
  {
   "source": [
    "## Commonly-used Tensor Constructors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "torch.ones(2, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "torch.zeros(2, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.0477, -0.9737, -0.0380,  0.0893,  0.9037],\n",
       "         [-1.1137, -1.6247,  0.1492, -2.1266,  0.9770],\n",
       "         [ 0.2443, -0.4878,  0.6149,  0.0773,  0.1182],\n",
       "         [-0.9318, -0.1522, -0.3544,  0.3717,  1.1152]],\n",
       "\n",
       "        [[-2.3148, -0.5276,  1.2426,  0.0250, -0.8366],\n",
       "         [ 0.0633,  1.0571, -0.8819,  0.2894, -1.8685],\n",
       "         [-0.5216,  0.6730,  0.1428,  1.2288, -0.1824],\n",
       "         [ 0.0847,  0.3545, -1.0089,  1.6480, -0.1725]]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "torch.randn(2, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "torch.tensor([[2, 3, 4]])"
   ]
  },
  {
   "source": [
    "## Commonly-used Tensor Operators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising with a type\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(4, 5)\n",
    "\n",
    "# Cloning\n",
    "B = A.clone()"
   ]
  },
  {
   "source": [
    "## Element-wise Operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 2.,  3.,  4.,  5.,  6.],\n",
       "         [ 7.,  8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15., 16.],\n",
       "         [17., 18., 19., 20., 21.]]),\n",
       " tensor([[ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.],\n",
       "         [15., 16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23., 24.]]),\n",
       " tensor([[0.8000, 1.0000, 1.2000, 1.4000, 1.6000],\n",
       "         [1.8000, 2.0000, 2.2000, 2.4000, 2.6000],\n",
       "         [2.8000, 3.0000, 3.2000, 3.4000, 3.6000],\n",
       "         [3.8000, 4.0000, 4.2000, 4.4000, 4.6000]]),\n",
       " tensor([[  0.,   1.,   4.,   9.,  16.],\n",
       "         [ 25.,  36.,  49.,  64.,  81.],\n",
       "         [100., 121., 144., 169., 196.],\n",
       "         [225., 256., 289., 324., 361.]]),\n",
       " tensor([[1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01],\n",
       "         [1.4841e+02, 4.0343e+02, 1.0966e+03, 2.9810e+03, 8.1031e+03],\n",
       "         [2.2026e+04, 5.9874e+04, 1.6275e+05, 4.4241e+05, 1.2026e+06],\n",
       "         [3.2690e+06, 8.8861e+06, 2.4155e+07, 6.5660e+07, 1.7848e+08]]),\n",
       " tensor([[  0.,   1.,   4.,   9.,  16.],\n",
       "         [ 25.,  36.,  49.,  64.,  81.],\n",
       "         [100., 121., 144., 169., 196.],\n",
       "         [225., 256., 289., 324., 361.]]),\n",
       " tensor([[0.0000e+00, 1.0000e+00, 1.6000e+01, 8.1000e+01, 2.5600e+02],\n",
       "         [6.2500e+02, 1.2960e+03, 2.4010e+03, 4.0960e+03, 6.5610e+03],\n",
       "         [1.0000e+04, 1.4641e+04, 2.0736e+04, 2.8561e+04, 3.8416e+04],\n",
       "         [5.0625e+04, 6.5536e+04, 8.3521e+04, 1.0498e+05, 1.3032e+05]]),\n",
       " tensor([[ 1.0000,  0.5403, -0.4161, -0.9900, -0.6536],\n",
       "         [ 0.2837,  0.9602,  0.7539, -0.1455, -0.9111],\n",
       "         [-0.8391,  0.0044,  0.8439,  0.9074,  0.1367],\n",
       "         [-0.7597, -0.9577, -0.2752,  0.6603,  0.9887]]))"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "A + 2, A + 5, (A + 4)/5, A * B, torch.exp(A), A ** 2, torch.pow(A, 4), torch.cos(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "A.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([10., 35., 60., 85.]), tensor([30., 34., 38., 42., 46.]))"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# Row-wise summation, Column-wise summation\n",
    "A.sum(dim=1), A.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.],\n",
       "         [15., 16., 17., 18., 19.],\n",
       "         [ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.],\n",
       "         [15., 16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  4.,  0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.,  5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14., 10., 11., 12., 13., 14.],\n",
       "         [15., 16., 17., 18., 19., 15., 16., 17., 18., 19.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    " # Concatination - row-wise, column-wise\n",
    " torch.cat((A, B), dim=0), torch.cat((A, B), dim=1)"
   ]
  },
  {
   "source": [
    "## Dot Product\n",
    "\n",
    "- Given two vectors, it calculates the sum over of the products of the elements at the same position\n",
    "- Given two vectors $x, y \\in R^d$ - their dot product will be $x^Ty = \\sum_{i=1}^d{x_iy_i}$\n",
    "- It can also be considered as their weighted sum of a vector for a given set of weights. It comes handly in calculating the regression score.  \n",
    "    - When the sum of all weights is one, then the dot product is expressed as weighted average\n",
    "    - After normalising two vectors to have the unit length, the dot product express the cosine of angle between them. It is useful for finding the similarity between two vectors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000]),\n",
       " tensor([ 5.0758,  1.0146, -1.6563, -2.9103, -0.5237]),\n",
       " tensor(-1.3124))"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "x = torch.arange(5, dtype=torch.float32)\n",
    "x /= x.sum()\n",
    "y = torch.randn(5)\n",
    "y /= y.sum()\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "source": [
    "## Matrix-Vector Products\n",
    "\n",
    "- Matrix and vector multiplication is very much like the matrix and matrix multiplication.\n",
    "- Given a matrix and vector, $A \\in R^{m*n}$ and $x \\in R^{n}$, the product $Ax$ will be: $Z \\in R^{m}$\n",
    "- Multiplication by $A \\in R^{m*n}$ projects vectors from their initial size to the size of the rows of the matrix: $R^n$ -> $R^m$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([4, 5]), torch.Size([5]))"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(4, -1)\n",
    "x = torch.arange(5)+1\n",
    "A.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]),\n",
       " tensor([1, 2, 3, 4, 5]),\n",
       " tensor([ 40, 115, 190, 265]))"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "A, x, torch.mv(A, x)"
   ]
  },
  {
   "source": [
    "## Matrix-Matrix Multiplication\n",
    "\n",
    "- In matrix-matrix multiplcation, the dot products of the rows of the first matrix and columns of the second matrix are calculated. Therefore, the number of number of columns in the first matrix should match the number of rows of the second matrix. \n",
    "- Assuming two matrices $A \\in R^{n*k}$ and $B \\in R^{q*j}$, then for multiplcation, k == q. \n",
    "- The result of multiplcation of A abd B will be $Z \\in R^{n*j}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]),\n",
       " torch.Size([4, 5]),\n",
       " tensor([[0, 1],\n",
       "         [2, 3],\n",
       "         [4, 5],\n",
       "         [6, 7],\n",
       "         [8, 9]]),\n",
       " torch.Size([5, 2]))"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(-1, 5)\n",
    "B = torch.arange(10).reshape(5, -1)\n",
    "A, A.shape, B, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 60,  70],\n",
       "         [160, 195],\n",
       "         [260, 320],\n",
       "         [360, 445]]),\n",
       " torch.Size([4, 2]))"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "Z = torch.mm(A, B)\n",
    "Z, Z.shape"
   ]
  },
  {
   "source": [
    "## Norms\n",
    "\n",
    "Norms are functions that converts a real or complex vector space to a non-negative real numbers that behaves in a certain way like the distance from the origin. These functions have an origin as zero and are mostly used in comparing numbers. \n",
    "\n",
    "- The euclidian distance of a vector from an origin is a norm, call ed the Euclidian norm or 2-norm ($L_1$) which may also be defined as the square root of the inner product of a vector itself. \n",
    "$$ ||x||_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$$\n",
    "- $L_1$ norm is expressed as the sum of the absolute values of the vector elements. \n",
    "$$ ||x||_1 = \\sum_{i=1}^n |x_i| $$\n",
    "- In Deep Learning, norms are used for comparing the model's predictions and the ground-truth values, and try to minimize the distance of the resulting difference from the origin i.e. push the value towards the origin. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Broadcasting Mechanism \n",
    "\n",
    "In maths, element-wise operation between two tensors can only happen when they both have the same size. However this is not the case with Python and Pytorch. Under certain conditions, even if their shapes differ, we can still perform element-wise operations by invoking the Broadcasting mechanism.\n",
    "\n",
    "The mechanism first increase the size of one of the tensors by copying elements to match the size of two tensors. Then it performs the element-wise operation. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[3],\n",
       "         [2],\n",
       "         [2]], dtype=torch.int32),\n",
       " tensor([[1, 1]], dtype=torch.int32),\n",
       " tensor([[4, 4],\n",
       "         [3, 3],\n",
       "         [3, 3]], dtype=torch.int32),\n",
       " tensor([[4, 4],\n",
       "         [3, 3],\n",
       "         [3, 3]], dtype=torch.int32))"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "A = torch.randn(3, 1).type(torch.int32)+2\n",
    "C = torch.ones(1, 2).type(torch.int32)\n",
    "A, C, C + A, A+C"
   ]
  },
  {
   "source": [
    "## Saving Memory\n",
    "\n",
    "Running operations can cause allocation of new memory to store the results of some operation instead of reusing existing ones. In Deep Learning, we normally deal with models that contains dozens of parameters and hundreds of rows of data. Taking extra unneccessary storage slow down the process. Therefore, we should do operations in a way that they use the in-place memory, and does not take new memory. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory Locations\n140304602130752 140304602131776\n140304602131776 140304662056000 False\n140304662056000 140304662056000 True\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(20).type(torch.float32).reshape(5, 4)\n",
    "Y = 10*X\n",
    "\n",
    "print(\"Memory Locations\")\n",
    "old_loc = id(Y)\n",
    "print(id(X), id(Y))\n",
    "\n",
    "# New memory operation\n",
    "Y = Y+X\n",
    "print(old_loc, id(Y), old_loc == id(Y))\n",
    "old_loc_2 = id(Y)\n",
    "\n",
    "# In-place operation\n",
    "Y += X\n",
    "print(old_loc_2, id(Y), old_loc_2==id(Y))\n"
   ]
  }
 ]
}