{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03af7543806d644da5d7678fad71032ad9229751404418c10020b39d91c5ca178",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "source": [
    "## Linear Regression\n",
    "\n",
    "Regression refers to a set of methods for modeling relationship between one or more independent variables and a dependent variable. When the relationship between variables is linear, then it is expressed as a Linear Regression. \n",
    "\n",
    "This relationship is expressed as a linear equation, $y = xw + b$, where x is the set of independent variables multiplied by some weights, b is a bias and y is the dependent variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linear Model\n",
    "\n",
    "When the inputs consist of $d$ parameters/features/attributes, our prediction $\\hat{y}$ is \n",
    "$$ \\hat{y} = w_1x_1 + ... + w_dx_d + b $$\n",
    "\n",
    "The above equation can also be expressed as \n",
    "$$ \\hat{y} = w^Tx+b $$\n",
    "where $x \\in R^{d}$ is a features vector and $w \\in R^{d}$ is a weights vector and their dot product will be the weighted sum of the product of feature vectors with the weights vector. \n",
    "\n",
    "Normally, the dataset consist of more than a single example, therefore there will be a features matrix not the features vector, where each row represents an observation or sample. \n",
    "\n",
    "The above equation will be - $\\hat{y} = Xw + b$, where $X \\in R^{n*d}$\n",
    "\n",
    "> Note: Broadcasting is applied during the summation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training Dataset\n",
    "\n",
    "To find the weights and bais of a model for regression, we need a Training dataset consisting of both the independent variables and a dependent variable. \n",
    "\n",
    "Given features of a training dataset X and corresponding labels y, the goal is to find $w$ and $b$ such that the model makes predictions with the least error. Therefore, we need to two things to accomplish this task. \n",
    "- A quality measure to find the accuracy of the model (hint: norms).\n",
    "- A procedure for updating the model to improve its quality."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loss Function\n",
    "\n",
    "Loss function is a non-negative function that is used for comparing the ground-truth values to the predicted values. Norms are normally used as loss functions, as they have the same origin and the lower the number they produce the better the predictions will be. \n",
    "\n",
    "The most popular function is a squared error, which is a $L_2$ norm:\n",
    "$$l^{(i)}(w,b) = \\frac{1}{2}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "TO measure the quality of the model with n training examples, we calculate the average of all the squared differences, and the loss function can be expressed as:\n",
    "$$L(w,b) = \\frac{1}{n} \\sum_{i=1}^nl^{(i)}(w,b) =  \\frac{1}{n}\\sum_{i=1}^n\\frac{1}{2} (w^Tx^{(i)} + n - y^{(i)})^2 $$ \n",
    "\n",
    "As training the model is an optimisation problem, we find parameters that minimise the loss function across all training examples: \n",
    "$$w^*,b^* = argmin_{w,b} L(w, b) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent (GD)\n",
    "\n",
    "There is an mathematical equation that can calculate the value of weights and bias depending on the data supplied, however it is too rigid and can only solve problem for a specific equation. In other words, for each model there is an equation to find weights and bias. This approach does not generalise for all the models and it is difficult to derive. \n",
    "\n",
    "Gradient Descent is a key technique used for optimising nearly any deep learning model. It is an algorithm that iteratively reduce the loss produced by the error by updating the parameters (weights and bias) in the direction of the minima of the loss function. \n",
    "\n",
    "We use partial derivative in this algorithm to calculate the change in loss function w.r.t the parameters:\n",
    "\n",
    "$$w = w - \\frac{\\eta}{N} \\sum_{i=1}^N \\triangledown_wl^{(i)}(w, b)$$\n",
    "$$b = b - \\frac{\\eta}{N} \\sum_{i=1}^N \\frac{\\delta l^{(i)}(w, b)}{\\delta b}$$\n",
    "\n",
    "where $\\eta$ is a positive scaler called the __Learning Rate__. We randomly initialise the values of the model paramters, and use Gradient Descent to produce the optimal sets of parameters.\n",
    "\n",
    "## Minibatch Stochastic Gradient Descent\n",
    "It is a type of gradient descent algorithm that only takes a random minibatch of samples every time it compute the weights. Normal Gradient Descent takes time to compute as it runs over the whole dataset in each training iteration, minibatch solves this problem by using only a small subset at a time, therefore minibatch takes comparatively lower time to find paramteres. The above equations can be updated to:\n",
    "$$w = w - \\frac{\\eta}{|\\beta|} \\sum_{i=1}^{|\\beta|} \\triangledown_wl^{(i)}(w, b)$$\n",
    "$$b = b - \\frac{\\eta}{|\\beta|} \\sum_{i=1}^{|\\beta|} \\frac{\\delta l^{(i)}(w, b)}{\\delta b}$$\n",
    "\n",
    "In this method, there are two minimum two hyperparamters required: Batchsize and Learning Rate. hyperparamter tuning is a process by which hyperparamters are chosen based on results on a separate __Validation__ set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Deep Learning Pipeline\n",
    "\n",
    "1. Create and read the dataset\n",
    "2. Prepare the modeling \n",
    "3. Initialise the parameters (Weights and Bias)\n",
    "4. Define the loss function (MSE)\n",
    "5. Choose the training algorithms (SGD|ADAM)\n",
    "6. Run the training method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Dataset\n",
    "\n",
    "def prepare_data(w, b, num_examples):\n",
    "    \"\"\" Generate y = Xw + b + noise \"\"\"\n",
    "    X = torch.normal(0, 1 ,(num_examples, len(w)))\n",
    "    y = torch.mm(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.size())\n",
    "    return X, y.reshape(-1, 1)\n",
    "\n",
    "true_w = torch.tensor([[2], [-3.3]])\n",
    "true_b = torch.tensor([4.5])\n",
    "features, labels = prepare_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(features, labels)\n",
    "batch_size = 10\n",
    "data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "num_of_inp, num_of_out = 2, 1\n",
    "net = torch.nn.Linear(num_of_inp, num_of_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Initialise the parameters\n",
    "net.weight.data.normal_(0, 0.01)\n",
    "net.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the loss function\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the training algorithms (SGD|ADAM)\n",
    "optimiser = torch.optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss 24.812777\n",
      "epoch 2, loss 15.937796\n",
      "epoch 3, loss 10.248366\n",
      "epoch 4, loss 6.597397\n",
      "epoch 5, loss 4.251769\n",
      "epoch 6, loss 2.743270\n",
      "epoch 7, loss 1.772013\n",
      "epoch 8, loss 1.145958\n",
      "epoch 9, loss 0.742014\n",
      "epoch 10, loss 0.481031\n",
      "epoch 11, loss 0.312233\n",
      "epoch 12, loss 0.202919\n",
      "epoch 13, loss 0.132049\n",
      "epoch 14, loss 0.086042\n",
      "epoch 15, loss 0.056144\n",
      "epoch 16, loss 0.036690\n",
      "epoch 17, loss 0.024015\n",
      "epoch 18, loss 0.015748\n",
      "epoch 19, loss 0.010351\n",
      "epoch 20, loss 0.006821\n",
      "epoch 21, loss 0.004510\n",
      "epoch 22, loss 0.002997\n",
      "epoch 23, loss 0.002004\n",
      "epoch 24, loss 0.001352\n",
      "epoch 25, loss 0.000923\n",
      "epoch 26, loss 0.000641\n",
      "epoch 27, loss 0.000455\n",
      "epoch 28, loss 0.000333\n",
      "epoch 29, loss 0.000252\n",
      "epoch 30, loss 0.000198\n",
      "epoch 31, loss 0.000163\n",
      "epoch 32, loss 0.000140\n",
      "epoch 33, loss 0.000124\n",
      "epoch 34, loss 0.000114\n",
      "epoch 35, loss 0.000107\n",
      "epoch 36, loss 0.000103\n",
      "epoch 37, loss 0.000100\n",
      "epoch 38, loss 0.000098\n",
      "epoch 39, loss 0.000096\n",
      "epoch 40, loss 0.000095\n",
      "epoch 41, loss 0.000095\n",
      "epoch 42, loss 0.000094\n",
      "epoch 43, loss 0.000094\n",
      "epoch 44, loss 0.000094\n",
      "epoch 45, loss 0.000094\n",
      "epoch 46, loss 0.000094\n",
      "epoch 47, loss 0.000094\n",
      "epoch 48, loss 0.000094\n",
      "epoch 49, loss 0.000094\n",
      "epoch 50, loss 0.000094\n"
     ]
    }
   ],
   "source": [
    "# Run the training method\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimiser.zero_grad()\n",
    "        l.backward()\n",
    "        optimiser.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f\"epoch {epoch+1}, loss {l:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True weights (tensor([[ 2.0000],\n        [-3.3000]])), Predicted weights (tensor([[ 2.0001, -3.3007]])), error in estimating w: tensor([[-5.7220e-05],\n        [ 6.5303e-04]])\nTrue bais (tensor([4.5000])), Predicted bais (tensor([4.4999])), error in estimating b: tensor([0.0001])\n"
     ]
    }
   ],
   "source": [
    "new_w = net.weight.data\n",
    "new_b = net.bias.data\n",
    "print(\"True weights ({}), Predicted weights ({}), error in estimating w: {}\".format(true_w, new_w, true_w - new_w.reshape(true_w.shape)))\n",
    "print(\"True bais ({}), Predicted bais ({}), error in estimating b: {}\".format(true_b, new_b, true_b - new_b.reshape(true_b.shape)))"
   ]
  }
 ]
}