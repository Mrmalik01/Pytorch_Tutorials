{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03af7543806d644da5d7678fad71032ad9229751404418c10020b39d91c5ca178",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "source": [
    "## Linear Regression\n",
    "\n",
    "Regression refers to a set of methods for modeling relationship between one or more independent variables and a dependent variable. When the relationship between variables is linear, then it is expressed as a Linear Regression. \n",
    "\n",
    "This relationship is expressed as a linear equation, $y = xw + b$, where x is the set of independent variables multiplied by some weights, b is a bias and y is the dependent variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linear Model\n",
    "\n",
    "When the inputs consist of $d$ parameters/features/attributes, our prediction $\\hat{y}$ is \n",
    "$$ \\hat{y} = w_1x_1 + ... + w_dx_d + b $$\n",
    "\n",
    "The above equation can also be expressed as \n",
    "$$ \\hat{y} = w^Tx+b $$\n",
    "where $x \\in R^{d}$ is a features vector and $w \\in R^{d}$ is a weights vector and their dot product will be the weighted sum of the product of feature vectors with the weights vector. \n",
    "\n",
    "Normally, the dataset consist of more than a single example, therefore there will be a features matrix not the features vector, where each row represents an observation or sample. \n",
    "\n",
    "The above equation will be - $\\hat{y} = Xw + b$, where $X \\in R^{n*d}$\n",
    "\n",
    "> Note: Broadcasting is applied during the summation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training Dataset\n",
    "\n",
    "To find the weights and bais of a model for regression, we need a Training dataset consisting of both the independent variables and a dependent variable. \n",
    "\n",
    "Given features of a training dataset X and corresponding labels y, the goal is to find $w$ and $b$ such that the model makes predictions with the least error. Therefore, we need to two things to accomplish this task. \n",
    "- A quality measure to find the accuracy of the model (hint: norms).\n",
    "- A procedure for updating the model to improve its quality."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loss Function\n",
    "\n",
    "Loss function is a non-negative function that is used for comparing the ground-truth values to the predicted values. Norms are normally used as loss functions, as they have the same origin and the lower the number they produce the better the predictions will be. \n",
    "\n",
    "The most popular function is a squared error, which is a $L_2$ norm:\n",
    "$$l^{(i)}(w,b) = \\frac{1}{2}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "TO measure the quality of the model with n training examples, we calculate the average of all the squared differences, and the loss function can be expressed as:\n",
    "$$L(w,b) = \\frac{1}{n} \\sum_{i=1}^nl^{(i)}(w,b) =  \\frac{1}{n}\\sum_{i=1}^n\\frac{1}{2} (w^Tx^{(i)} + n - y^{(i)})^2 $$ \n",
    "\n",
    "As training the model is an optimisation problem, we find parameters that minimise the loss function across all training examples: \n",
    "$$w^*,b^* = argmin_{w,b} L(w, b) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent (GD)\n",
    "\n",
    "There is an mathematical equation that can calculate the value of weights and bias depending on the data supplied, however it is too rigid and can only solve problem for a specific equation. In other words, for each model there is an equation to find weights and bias. This approach does not generalise for all the models and it is difficult to derive. \n",
    "\n",
    "Gradient Descent is a key technique used for optimising nearly any deep learning model. It is an algorithm that iteratively reduce the loss produced by the error by updating the parameters (weights and bias) in the direction of the minima of the loss function. \n",
    "\n",
    "We use partial derivative in this algorithm to calculate the change in loss function w.r.t the parameters:\n",
    "\n",
    "$$w = w - \\frac{\\eta}{N} \\sum_{i=1}^N \\triangledown_wl^{(i)}(w, b)$$\n",
    "$$b = b - \\frac{\\eta}{N} \\sum_{i=1}^N \\frac{\\delta l^{(i)}(w, b)}{\\delta b}$$\n",
    "\n",
    "where $\\eta$ is a positive scaler called the __Learning Rate__. We randomly initialise the values of the model paramters, and use Gradient Descent to produce the optimal sets of parameters.\n",
    "\n",
    "## Minibatch Stochastic Gradient Descent\n",
    "It is a type of gradient descent algorithm that only takes a random minibatch of samples every time it compute the weights. Normal Gradient Descent takes time to compute as it runs over the whole dataset in each training iteration, minibatch solves this problem by using only a small subset at a time, therefore minibatch takes comparatively lower time to find paramteres. The above equations can be updated to:\n",
    "$$w = w - \\frac{\\eta}{|\\beta|} \\sum_{i=1}^{|\\beta|} \\triangledown_wl^{(i)}(w, b)$$\n",
    "$$b = b - \\frac{\\eta}{|\\beta|} \\sum_{i=1}^{|\\beta|} \\frac{\\delta l^{(i)}(w, b)}{\\delta b}$$\n",
    "\n",
    "In this method, there are two minimum two hyperparamters required: Batchsize and Learning Rate. hyperparamter tuning is a process by which hyperparamters are chosen based on results on a separate __Validation__ set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Deep Learning Pipeline\n",
    "\n",
    "1. Create and read the dataset\n",
    "2. Prepare the modeling \n",
    "3. Initialise the parameters (Weights and Bias)\n",
    "4. Define the loss function (MSE)\n",
    "5. Choose the training algorithms (SGD|ADAM)\n",
    "6. Run the training method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Dataset\n",
    "\n",
    "def prepare_data(w, b, num_examples):\n",
    "    \"\"\" Generate y = Xw + b + noise \"\"\"\n",
    "    X = torch.normal(0, 1 ,(num_examples, len(w)))\n",
    "    y = torch.mm(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.size())\n",
    "    return X, y.reshape(-1, 1)\n",
    "\n",
    "true_w = torch.tensor([[2], [-3.3]])\n",
    "true_b = torch.tensor([4.5])\n",
    "features, labels = prepare_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(features, labels)\n",
    "batch_size = 10\n",
    "data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "num_of_inp, num_of_out = 2, 1\n",
    "net = torch.nn.Linear(num_of_inp, num_of_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Initialise the parameters\n",
    "net.weight.data.normal_(0, 0.01)\n",
    "net.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the loss function\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the training algorithms (SGD|ADAM)\n",
    "optimiser = torch.optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "00101\n",
      "epoch 1232, loss 0.000101\n",
      "epoch 1233, loss 0.000101\n",
      "epoch 1234, loss 0.000101\n",
      "epoch 1235, loss 0.000101\n",
      "epoch 1236, loss 0.000101\n",
      "epoch 1237, loss 0.000101\n",
      "epoch 1238, loss 0.000101\n",
      "epoch 1239, loss 0.000101\n",
      "epoch 1240, loss 0.000101\n",
      "epoch 1241, loss 0.000101\n",
      "epoch 1242, loss 0.000101\n",
      "epoch 1243, loss 0.000101\n",
      "epoch 1244, loss 0.000101\n",
      "epoch 1245, loss 0.000101\n",
      "epoch 1246, loss 0.000101\n",
      "epoch 1247, loss 0.000101\n",
      "epoch 1248, loss 0.000101\n",
      "epoch 1249, loss 0.000101\n",
      "epoch 1250, loss 0.000101\n",
      "epoch 1251, loss 0.000101\n",
      "epoch 1252, loss 0.000101\n",
      "epoch 1253, loss 0.000101\n",
      "epoch 1254, loss 0.000101\n",
      "epoch 1255, loss 0.000101\n",
      "epoch 1256, loss 0.000101\n",
      "epoch 1257, loss 0.000101\n",
      "epoch 1258, loss 0.000101\n",
      "epoch 1259, loss 0.000101\n",
      "epoch 1260, loss 0.000101\n",
      "epoch 1261, loss 0.000101\n",
      "epoch 1262, loss 0.000101\n",
      "epoch 1263, loss 0.000101\n",
      "epoch 1264, loss 0.000101\n",
      "epoch 1265, loss 0.000101\n",
      "epoch 1266, loss 0.000101\n",
      "epoch 1267, loss 0.000101\n",
      "epoch 1268, loss 0.000101\n",
      "epoch 1269, loss 0.000101\n",
      "epoch 1270, loss 0.000101\n",
      "epoch 1271, loss 0.000101\n",
      "epoch 1272, loss 0.000101\n",
      "epoch 1273, loss 0.000101\n",
      "epoch 1274, loss 0.000101\n",
      "epoch 1275, loss 0.000101\n",
      "epoch 1276, loss 0.000101\n",
      "epoch 1277, loss 0.000101\n",
      "epoch 1278, loss 0.000101\n",
      "epoch 1279, loss 0.000101\n",
      "epoch 1280, loss 0.000101\n",
      "epoch 1281, loss 0.000101\n",
      "epoch 1282, loss 0.000101\n",
      "epoch 1283, loss 0.000101\n",
      "epoch 1284, loss 0.000101\n",
      "epoch 1285, loss 0.000101\n",
      "epoch 1286, loss 0.000101\n",
      "epoch 1287, loss 0.000101\n",
      "epoch 1288, loss 0.000101\n",
      "epoch 1289, loss 0.000101\n",
      "epoch 1290, loss 0.000101\n",
      "epoch 1291, loss 0.000101\n",
      "epoch 1292, loss 0.000101\n",
      "epoch 1293, loss 0.000101\n",
      "epoch 1294, loss 0.000101\n",
      "epoch 1295, loss 0.000101\n",
      "epoch 1296, loss 0.000101\n",
      "epoch 1297, loss 0.000101\n",
      "epoch 1298, loss 0.000101\n",
      "epoch 1299, loss 0.000101\n",
      "epoch 1300, loss 0.000101\n",
      "epoch 1301, loss 0.000101\n",
      "epoch 1302, loss 0.000101\n",
      "epoch 1303, loss 0.000101\n",
      "epoch 1304, loss 0.000101\n",
      "epoch 1305, loss 0.000101\n",
      "epoch 1306, loss 0.000101\n",
      "epoch 1307, loss 0.000101\n",
      "epoch 1308, loss 0.000101\n",
      "epoch 1309, loss 0.000101\n",
      "epoch 1310, loss 0.000101\n",
      "epoch 1311, loss 0.000101\n",
      "epoch 1312, loss 0.000101\n",
      "epoch 1313, loss 0.000101\n",
      "epoch 1314, loss 0.000101\n",
      "epoch 1315, loss 0.000101\n",
      "epoch 1316, loss 0.000101\n",
      "epoch 1317, loss 0.000101\n",
      "epoch 1318, loss 0.000101\n",
      "epoch 1319, loss 0.000101\n",
      "epoch 1320, loss 0.000101\n",
      "epoch 1321, loss 0.000101\n",
      "epoch 1322, loss 0.000101\n",
      "epoch 1323, loss 0.000101\n",
      "epoch 1324, loss 0.000101\n",
      "epoch 1325, loss 0.000101\n",
      "epoch 1326, loss 0.000101\n",
      "epoch 1327, loss 0.000101\n",
      "epoch 1328, loss 0.000101\n",
      "epoch 1329, loss 0.000101\n",
      "epoch 1330, loss 0.000101\n",
      "epoch 1331, loss 0.000101\n",
      "epoch 1332, loss 0.000101\n",
      "epoch 1333, loss 0.000101\n",
      "epoch 1334, loss 0.000101\n",
      "epoch 1335, loss 0.000101\n",
      "epoch 1336, loss 0.000101\n",
      "epoch 1337, loss 0.000101\n",
      "epoch 1338, loss 0.000101\n",
      "epoch 1339, loss 0.000101\n",
      "epoch 1340, loss 0.000101\n",
      "epoch 1341, loss 0.000101\n",
      "epoch 1342, loss 0.000101\n",
      "epoch 1343, loss 0.000101\n",
      "epoch 1344, loss 0.000101\n",
      "epoch 1345, loss 0.000101\n",
      "epoch 1346, loss 0.000101\n",
      "epoch 1347, loss 0.000101\n",
      "epoch 1348, loss 0.000101\n",
      "epoch 1349, loss 0.000101\n",
      "epoch 1350, loss 0.000101\n",
      "epoch 1351, loss 0.000101\n",
      "epoch 1352, loss 0.000101\n",
      "epoch 1353, loss 0.000101\n",
      "epoch 1354, loss 0.000101\n",
      "epoch 1355, loss 0.000101\n",
      "epoch 1356, loss 0.000101\n",
      "epoch 1357, loss 0.000101\n",
      "epoch 1358, loss 0.000101\n",
      "epoch 1359, loss 0.000101\n",
      "epoch 1360, loss 0.000101\n",
      "epoch 1361, loss 0.000101\n",
      "epoch 1362, loss 0.000101\n",
      "epoch 1363, loss 0.000101\n",
      "epoch 1364, loss 0.000101\n",
      "epoch 1365, loss 0.000101\n",
      "epoch 1366, loss 0.000101\n",
      "epoch 1367, loss 0.000101\n",
      "epoch 1368, loss 0.000101\n",
      "epoch 1369, loss 0.000101\n",
      "epoch 1370, loss 0.000101\n",
      "epoch 1371, loss 0.000101\n",
      "epoch 1372, loss 0.000101\n",
      "epoch 1373, loss 0.000101\n",
      "epoch 1374, loss 0.000101\n",
      "epoch 1375, loss 0.000101\n",
      "epoch 1376, loss 0.000101\n",
      "epoch 1377, loss 0.000101\n",
      "epoch 1378, loss 0.000101\n",
      "epoch 1379, loss 0.000101\n",
      "epoch 1380, loss 0.000101\n",
      "epoch 1381, loss 0.000101\n",
      "epoch 1382, loss 0.000101\n",
      "epoch 1383, loss 0.000101\n",
      "epoch 1384, loss 0.000101\n",
      "epoch 1385, loss 0.000101\n",
      "epoch 1386, loss 0.000101\n",
      "epoch 1387, loss 0.000101\n",
      "epoch 1388, loss 0.000101\n",
      "epoch 1389, loss 0.000101\n",
      "epoch 1390, loss 0.000101\n",
      "epoch 1391, loss 0.000101\n",
      "epoch 1392, loss 0.000101\n",
      "epoch 1393, loss 0.000101\n",
      "epoch 1394, loss 0.000101\n",
      "epoch 1395, loss 0.000101\n",
      "epoch 1396, loss 0.000101\n",
      "epoch 1397, loss 0.000101\n",
      "epoch 1398, loss 0.000101\n",
      "epoch 1399, loss 0.000101\n",
      "epoch 1400, loss 0.000101\n",
      "epoch 1401, loss 0.000101\n",
      "epoch 1402, loss 0.000101\n",
      "epoch 1403, loss 0.000101\n",
      "epoch 1404, loss 0.000101\n",
      "epoch 1405, loss 0.000101\n",
      "epoch 1406, loss 0.000101\n",
      "epoch 1407, loss 0.000101\n",
      "epoch 1408, loss 0.000101\n",
      "epoch 1409, loss 0.000101\n",
      "epoch 1410, loss 0.000101\n",
      "epoch 1411, loss 0.000101\n",
      "epoch 1412, loss 0.000101\n",
      "epoch 1413, loss 0.000101\n",
      "epoch 1414, loss 0.000101\n",
      "epoch 1415, loss 0.000101\n",
      "epoch 1416, loss 0.000101\n",
      "epoch 1417, loss 0.000101\n",
      "epoch 1418, loss 0.000101\n",
      "epoch 1419, loss 0.000101\n",
      "epoch 1420, loss 0.000101\n",
      "epoch 1421, loss 0.000101\n",
      "epoch 1422, loss 0.000101\n",
      "epoch 1423, loss 0.000101\n",
      "epoch 1424, loss 0.000101\n",
      "epoch 1425, loss 0.000101\n",
      "epoch 1426, loss 0.000101\n",
      "epoch 1427, loss 0.000101\n",
      "epoch 1428, loss 0.000101\n",
      "epoch 1429, loss 0.000101\n",
      "epoch 1430, loss 0.000101\n",
      "epoch 1431, loss 0.000101\n",
      "epoch 1432, loss 0.000101\n",
      "epoch 1433, loss 0.000101\n",
      "epoch 1434, loss 0.000101\n",
      "epoch 1435, loss 0.000101\n",
      "epoch 1436, loss 0.000101\n",
      "epoch 1437, loss 0.000101\n",
      "epoch 1438, loss 0.000101\n",
      "epoch 1439, loss 0.000101\n",
      "epoch 1440, loss 0.000101\n",
      "epoch 1441, loss 0.000101\n",
      "epoch 1442, loss 0.000101\n",
      "epoch 1443, loss 0.000101\n",
      "epoch 1444, loss 0.000101\n",
      "epoch 1445, loss 0.000101\n",
      "epoch 1446, loss 0.000101\n",
      "epoch 1447, loss 0.000101\n",
      "epoch 1448, loss 0.000101\n",
      "epoch 1449, loss 0.000101\n",
      "epoch 1450, loss 0.000101\n",
      "epoch 1451, loss 0.000101\n",
      "epoch 1452, loss 0.000101\n",
      "epoch 1453, loss 0.000101\n",
      "epoch 1454, loss 0.000101\n",
      "epoch 1455, loss 0.000101\n",
      "epoch 1456, loss 0.000101\n",
      "epoch 1457, loss 0.000101\n",
      "epoch 1458, loss 0.000101\n",
      "epoch 1459, loss 0.000101\n",
      "epoch 1460, loss 0.000101\n",
      "epoch 1461, loss 0.000101\n",
      "epoch 1462, loss 0.000101\n",
      "epoch 1463, loss 0.000101\n",
      "epoch 1464, loss 0.000101\n",
      "epoch 1465, loss 0.000101\n",
      "epoch 1466, loss 0.000101\n",
      "epoch 1467, loss 0.000101\n",
      "epoch 1468, loss 0.000101\n",
      "epoch 1469, loss 0.000101\n",
      "epoch 1470, loss 0.000101\n",
      "epoch 1471, loss 0.000101\n",
      "epoch 1472, loss 0.000101\n",
      "epoch 1473, loss 0.000101\n",
      "epoch 1474, loss 0.000101\n",
      "epoch 1475, loss 0.000101\n",
      "epoch 1476, loss 0.000101\n",
      "epoch 1477, loss 0.000101\n",
      "epoch 1478, loss 0.000101\n",
      "epoch 1479, loss 0.000101\n",
      "epoch 1480, loss 0.000101\n",
      "epoch 1481, loss 0.000101\n",
      "epoch 1482, loss 0.000101\n",
      "epoch 1483, loss 0.000101\n",
      "epoch 1484, loss 0.000101\n",
      "epoch 1485, loss 0.000101\n",
      "epoch 1486, loss 0.000101\n",
      "epoch 1487, loss 0.000101\n",
      "epoch 1488, loss 0.000101\n",
      "epoch 1489, loss 0.000101\n",
      "epoch 1490, loss 0.000101\n",
      "epoch 1491, loss 0.000101\n",
      "epoch 1492, loss 0.000101\n",
      "epoch 1493, loss 0.000101\n",
      "epoch 1494, loss 0.000101\n",
      "epoch 1495, loss 0.000101\n",
      "epoch 1496, loss 0.000101\n",
      "epoch 1497, loss 0.000101\n",
      "epoch 1498, loss 0.000101\n",
      "epoch 1499, loss 0.000101\n",
      "epoch 1500, loss 0.000101\n",
      "epoch 1501, loss 0.000101\n",
      "epoch 1502, loss 0.000101\n",
      "epoch 1503, loss 0.000101\n",
      "epoch 1504, loss 0.000101\n",
      "epoch 1505, loss 0.000101\n",
      "epoch 1506, loss 0.000101\n",
      "epoch 1507, loss 0.000101\n",
      "epoch 1508, loss 0.000101\n",
      "epoch 1509, loss 0.000101\n",
      "epoch 1510, loss 0.000101\n",
      "epoch 1511, loss 0.000101\n",
      "epoch 1512, loss 0.000101\n",
      "epoch 1513, loss 0.000101\n",
      "epoch 1514, loss 0.000101\n",
      "epoch 1515, loss 0.000101\n",
      "epoch 1516, loss 0.000101\n",
      "epoch 1517, loss 0.000101\n",
      "epoch 1518, loss 0.000101\n",
      "epoch 1519, loss 0.000101\n",
      "epoch 1520, loss 0.000101\n",
      "epoch 1521, loss 0.000101\n",
      "epoch 1522, loss 0.000101\n",
      "epoch 1523, loss 0.000101\n",
      "epoch 1524, loss 0.000101\n",
      "epoch 1525, loss 0.000101\n",
      "epoch 1526, loss 0.000101\n",
      "epoch 1527, loss 0.000101\n",
      "epoch 1528, loss 0.000101\n",
      "epoch 1529, loss 0.000101\n",
      "epoch 1530, loss 0.000101\n",
      "epoch 1531, loss 0.000101\n",
      "epoch 1532, loss 0.000101\n",
      "epoch 1533, loss 0.000101\n",
      "epoch 1534, loss 0.000101\n",
      "epoch 1535, loss 0.000101\n",
      "epoch 1536, loss 0.000101\n",
      "epoch 1537, loss 0.000101\n",
      "epoch 1538, loss 0.000101\n",
      "epoch 1539, loss 0.000101\n",
      "epoch 1540, loss 0.000101\n",
      "epoch 1541, loss 0.000101\n",
      "epoch 1542, loss 0.000101\n",
      "epoch 1543, loss 0.000101\n",
      "epoch 1544, loss 0.000101\n",
      "epoch 1545, loss 0.000101\n",
      "epoch 1546, loss 0.000101\n",
      "epoch 1547, loss 0.000101\n",
      "epoch 1548, loss 0.000101\n",
      "epoch 1549, loss 0.000101\n",
      "epoch 1550, loss 0.000101\n",
      "epoch 1551, loss 0.000101\n",
      "epoch 1552, loss 0.000101\n",
      "epoch 1553, loss 0.000101\n",
      "epoch 1554, loss 0.000101\n",
      "epoch 1555, loss 0.000101\n",
      "epoch 1556, loss 0.000101\n",
      "epoch 1557, loss 0.000101\n",
      "epoch 1558, loss 0.000101\n",
      "epoch 1559, loss 0.000101\n",
      "epoch 1560, loss 0.000101\n",
      "epoch 1561, loss 0.000101\n",
      "epoch 1562, loss 0.000101\n",
      "epoch 1563, loss 0.000101\n",
      "epoch 1564, loss 0.000101\n",
      "epoch 1565, loss 0.000101\n",
      "epoch 1566, loss 0.000101\n",
      "epoch 1567, loss 0.000101\n",
      "epoch 1568, loss 0.000101\n",
      "epoch 1569, loss 0.000101\n",
      "epoch 1570, loss 0.000101\n",
      "epoch 1571, loss 0.000101\n",
      "epoch 1572, loss 0.000101\n",
      "epoch 1573, loss 0.000101\n",
      "epoch 1574, loss 0.000101\n",
      "epoch 1575, loss 0.000101\n",
      "epoch 1576, loss 0.000101\n",
      "epoch 1577, loss 0.000101\n",
      "epoch 1578, loss 0.000101\n",
      "epoch 1579, loss 0.000101\n",
      "epoch 1580, loss 0.000101\n",
      "epoch 1581, loss 0.000101\n",
      "epoch 1582, loss 0.000101\n",
      "epoch 1583, loss 0.000101\n",
      "epoch 1584, loss 0.000101\n",
      "epoch 1585, loss 0.000101\n",
      "epoch 1586, loss 0.000101\n",
      "epoch 1587, loss 0.000101\n",
      "epoch 1588, loss 0.000101\n",
      "epoch 1589, loss 0.000101\n",
      "epoch 1590, loss 0.000101\n",
      "epoch 1591, loss 0.000101\n",
      "epoch 1592, loss 0.000101\n",
      "epoch 1593, loss 0.000101\n",
      "epoch 1594, loss 0.000101\n",
      "epoch 1595, loss 0.000101\n",
      "epoch 1596, loss 0.000101\n",
      "epoch 1597, loss 0.000101\n",
      "epoch 1598, loss 0.000101\n",
      "epoch 1599, loss 0.000101\n",
      "epoch 1600, loss 0.000101\n",
      "epoch 1601, loss 0.000101\n",
      "epoch 1602, loss 0.000101\n",
      "epoch 1603, loss 0.000101\n",
      "epoch 1604, loss 0.000101\n",
      "epoch 1605, loss 0.000101\n",
      "epoch 1606, loss 0.000101\n",
      "epoch 1607, loss 0.000101\n",
      "epoch 1608, loss 0.000101\n",
      "epoch 1609, loss 0.000101\n",
      "epoch 1610, loss 0.000101\n",
      "epoch 1611, loss 0.000101\n",
      "epoch 1612, loss 0.000101\n",
      "epoch 1613, loss 0.000101\n",
      "epoch 1614, loss 0.000101\n",
      "epoch 1615, loss 0.000101\n",
      "epoch 1616, loss 0.000101\n",
      "epoch 1617, loss 0.000101\n",
      "epoch 1618, loss 0.000101\n",
      "epoch 1619, loss 0.000101\n",
      "epoch 1620, loss 0.000101\n",
      "epoch 1621, loss 0.000101\n",
      "epoch 1622, loss 0.000101\n",
      "epoch 1623, loss 0.000101\n",
      "epoch 1624, loss 0.000101\n",
      "epoch 1625, loss 0.000101\n",
      "epoch 1626, loss 0.000101\n",
      "epoch 1627, loss 0.000101\n",
      "epoch 1628, loss 0.000101\n",
      "epoch 1629, loss 0.000101\n",
      "epoch 1630, loss 0.000101\n",
      "epoch 1631, loss 0.000101\n",
      "epoch 1632, loss 0.000101\n",
      "epoch 1633, loss 0.000101\n",
      "epoch 1634, loss 0.000101\n",
      "epoch 1635, loss 0.000101\n",
      "epoch 1636, loss 0.000101\n",
      "epoch 1637, loss 0.000101\n",
      "epoch 1638, loss 0.000101\n",
      "epoch 1639, loss 0.000101\n",
      "epoch 1640, loss 0.000101\n",
      "epoch 1641, loss 0.000101\n",
      "epoch 1642, loss 0.000101\n",
      "epoch 1643, loss 0.000101\n",
      "epoch 1644, loss 0.000101\n",
      "epoch 1645, loss 0.000101\n",
      "epoch 1646, loss 0.000101\n",
      "epoch 1647, loss 0.000101\n",
      "epoch 1648, loss 0.000101\n",
      "epoch 1649, loss 0.000101\n",
      "epoch 1650, loss 0.000101\n",
      "epoch 1651, loss 0.000101\n",
      "epoch 1652, loss 0.000101\n",
      "epoch 1653, loss 0.000101\n",
      "epoch 1654, loss 0.000101\n",
      "epoch 1655, loss 0.000101\n",
      "epoch 1656, loss 0.000101\n",
      "epoch 1657, loss 0.000101\n",
      "epoch 1658, loss 0.000101\n",
      "epoch 1659, loss 0.000101\n",
      "epoch 1660, loss 0.000101\n",
      "epoch 1661, loss 0.000101\n",
      "epoch 1662, loss 0.000101\n",
      "epoch 1663, loss 0.000101\n",
      "epoch 1664, loss 0.000101\n",
      "epoch 1665, loss 0.000101\n",
      "epoch 1666, loss 0.000101\n",
      "epoch 1667, loss 0.000101\n",
      "epoch 1668, loss 0.000101\n",
      "epoch 1669, loss 0.000101\n",
      "epoch 1670, loss 0.000101\n",
      "epoch 1671, loss 0.000101\n",
      "epoch 1672, loss 0.000101\n",
      "epoch 1673, loss 0.000101\n",
      "epoch 1674, loss 0.000101\n",
      "epoch 1675, loss 0.000101\n",
      "epoch 1676, loss 0.000101\n",
      "epoch 1677, loss 0.000101\n",
      "epoch 1678, loss 0.000101\n",
      "epoch 1679, loss 0.000101\n",
      "epoch 1680, loss 0.000101\n",
      "epoch 1681, loss 0.000101\n",
      "epoch 1682, loss 0.000101\n",
      "epoch 1683, loss 0.000101\n",
      "epoch 1684, loss 0.000101\n",
      "epoch 1685, loss 0.000101\n",
      "epoch 1686, loss 0.000101\n",
      "epoch 1687, loss 0.000101\n",
      "epoch 1688, loss 0.000101\n",
      "epoch 1689, loss 0.000101\n",
      "epoch 1690, loss 0.000101\n",
      "epoch 1691, loss 0.000101\n",
      "epoch 1692, loss 0.000101\n",
      "epoch 1693, loss 0.000101\n",
      "epoch 1694, loss 0.000101\n",
      "epoch 1695, loss 0.000101\n",
      "epoch 1696, loss 0.000101\n",
      "epoch 1697, loss 0.000101\n",
      "epoch 1698, loss 0.000101\n",
      "epoch 1699, loss 0.000101\n",
      "epoch 1700, loss 0.000101\n",
      "epoch 1701, loss 0.000101\n",
      "epoch 1702, loss 0.000101\n",
      "epoch 1703, loss 0.000101\n",
      "epoch 1704, loss 0.000101\n",
      "epoch 1705, loss 0.000101\n",
      "epoch 1706, loss 0.000101\n",
      "epoch 1707, loss 0.000101\n",
      "epoch 1708, loss 0.000101\n",
      "epoch 1709, loss 0.000101\n",
      "epoch 1710, loss 0.000101\n",
      "epoch 1711, loss 0.000101\n",
      "epoch 1712, loss 0.000101\n",
      "epoch 1713, loss 0.000101\n",
      "epoch 1714, loss 0.000101\n",
      "epoch 1715, loss 0.000101\n",
      "epoch 1716, loss 0.000101\n",
      "epoch 1717, loss 0.000101\n",
      "epoch 1718, loss 0.000101\n",
      "epoch 1719, loss 0.000101\n",
      "epoch 1720, loss 0.000101\n",
      "epoch 1721, loss 0.000101\n",
      "epoch 1722, loss 0.000101\n",
      "epoch 1723, loss 0.000101\n",
      "epoch 1724, loss 0.000101\n",
      "epoch 1725, loss 0.000101\n",
      "epoch 1726, loss 0.000101\n",
      "epoch 1727, loss 0.000101\n",
      "epoch 1728, loss 0.000101\n",
      "epoch 1729, loss 0.000101\n",
      "epoch 1730, loss 0.000101\n",
      "epoch 1731, loss 0.000101\n",
      "epoch 1732, loss 0.000101\n",
      "epoch 1733, loss 0.000101\n",
      "epoch 1734, loss 0.000101\n",
      "epoch 1735, loss 0.000101\n",
      "epoch 1736, loss 0.000101\n",
      "epoch 1737, loss 0.000101\n",
      "epoch 1738, loss 0.000101\n",
      "epoch 1739, loss 0.000101\n",
      "epoch 1740, loss 0.000101\n",
      "epoch 1741, loss 0.000101\n",
      "epoch 1742, loss 0.000101\n",
      "epoch 1743, loss 0.000101\n",
      "epoch 1744, loss 0.000101\n",
      "epoch 1745, loss 0.000101\n",
      "epoch 1746, loss 0.000101\n",
      "epoch 1747, loss 0.000101\n",
      "epoch 1748, loss 0.000101\n",
      "epoch 1749, loss 0.000101\n",
      "epoch 1750, loss 0.000101\n",
      "epoch 1751, loss 0.000101\n",
      "epoch 1752, loss 0.000101\n",
      "epoch 1753, loss 0.000101\n",
      "epoch 1754, loss 0.000101\n",
      "epoch 1755, loss 0.000101\n",
      "epoch 1756, loss 0.000101\n",
      "epoch 1757, loss 0.000101\n",
      "epoch 1758, loss 0.000101\n",
      "epoch 1759, loss 0.000101\n",
      "epoch 1760, loss 0.000101\n",
      "epoch 1761, loss 0.000101\n",
      "epoch 1762, loss 0.000101\n",
      "epoch 1763, loss 0.000101\n",
      "epoch 1764, loss 0.000101\n",
      "epoch 1765, loss 0.000101\n",
      "epoch 1766, loss 0.000101\n",
      "epoch 1767, loss 0.000101\n",
      "epoch 1768, loss 0.000101\n",
      "epoch 1769, loss 0.000101\n",
      "epoch 1770, loss 0.000101\n",
      "epoch 1771, loss 0.000101\n",
      "epoch 1772, loss 0.000101\n",
      "epoch 1773, loss 0.000101\n",
      "epoch 1774, loss 0.000101\n",
      "epoch 1775, loss 0.000101\n",
      "epoch 1776, loss 0.000101\n",
      "epoch 1777, loss 0.000101\n",
      "epoch 1778, loss 0.000101\n",
      "epoch 1779, loss 0.000101\n",
      "epoch 1780, loss 0.000101\n",
      "epoch 1781, loss 0.000101\n",
      "epoch 1782, loss 0.000101\n",
      "epoch 1783, loss 0.000101\n",
      "epoch 1784, loss 0.000101\n",
      "epoch 1785, loss 0.000101\n",
      "epoch 1786, loss 0.000101\n",
      "epoch 1787, loss 0.000101\n",
      "epoch 1788, loss 0.000101\n",
      "epoch 1789, loss 0.000101\n",
      "epoch 1790, loss 0.000101\n",
      "epoch 1791, loss 0.000101\n",
      "epoch 1792, loss 0.000101\n",
      "epoch 1793, loss 0.000101\n",
      "epoch 1794, loss 0.000101\n",
      "epoch 1795, loss 0.000101\n",
      "epoch 1796, loss 0.000101\n",
      "epoch 1797, loss 0.000101\n",
      "epoch 1798, loss 0.000101\n",
      "epoch 1799, loss 0.000101\n",
      "epoch 1800, loss 0.000101\n",
      "epoch 1801, loss 0.000101\n",
      "epoch 1802, loss 0.000101\n",
      "epoch 1803, loss 0.000101\n",
      "epoch 1804, loss 0.000101\n",
      "epoch 1805, loss 0.000101\n",
      "epoch 1806, loss 0.000101\n",
      "epoch 1807, loss 0.000101\n",
      "epoch 1808, loss 0.000101\n",
      "epoch 1809, loss 0.000101\n",
      "epoch 1810, loss 0.000101\n",
      "epoch 1811, loss 0.000101\n",
      "epoch 1812, loss 0.000101\n",
      "epoch 1813, loss 0.000101\n",
      "epoch 1814, loss 0.000101\n",
      "epoch 1815, loss 0.000101\n",
      "epoch 1816, loss 0.000101\n",
      "epoch 1817, loss 0.000101\n",
      "epoch 1818, loss 0.000101\n",
      "epoch 1819, loss 0.000101\n",
      "epoch 1820, loss 0.000101\n",
      "epoch 1821, loss 0.000101\n",
      "epoch 1822, loss 0.000101\n",
      "epoch 1823, loss 0.000101\n",
      "epoch 1824, loss 0.000101\n",
      "epoch 1825, loss 0.000101\n",
      "epoch 1826, loss 0.000101\n",
      "epoch 1827, loss 0.000101\n",
      "epoch 1828, loss 0.000101\n",
      "epoch 1829, loss 0.000101\n",
      "epoch 1830, loss 0.000101\n",
      "epoch 1831, loss 0.000101\n",
      "epoch 1832, loss 0.000101\n",
      "epoch 1833, loss 0.000101\n",
      "epoch 1834, loss 0.000101\n",
      "epoch 1835, loss 0.000101\n",
      "epoch 1836, loss 0.000101\n",
      "epoch 1837, loss 0.000101\n",
      "epoch 1838, loss 0.000101\n",
      "epoch 1839, loss 0.000101\n",
      "epoch 1840, loss 0.000101\n",
      "epoch 1841, loss 0.000101\n",
      "epoch 1842, loss 0.000101\n",
      "epoch 1843, loss 0.000101\n",
      "epoch 1844, loss 0.000101\n",
      "epoch 1845, loss 0.000101\n",
      "epoch 1846, loss 0.000101\n",
      "epoch 1847, loss 0.000101\n",
      "epoch 1848, loss 0.000101\n",
      "epoch 1849, loss 0.000101\n",
      "epoch 1850, loss 0.000101\n",
      "epoch 1851, loss 0.000101\n",
      "epoch 1852, loss 0.000101\n",
      "epoch 1853, loss 0.000101\n",
      "epoch 1854, loss 0.000101\n",
      "epoch 1855, loss 0.000101\n",
      "epoch 1856, loss 0.000101\n",
      "epoch 1857, loss 0.000101\n",
      "epoch 1858, loss 0.000101\n",
      "epoch 1859, loss 0.000101\n",
      "epoch 1860, loss 0.000101\n",
      "epoch 1861, loss 0.000101\n",
      "epoch 1862, loss 0.000101\n",
      "epoch 1863, loss 0.000101\n",
      "epoch 1864, loss 0.000101\n",
      "epoch 1865, loss 0.000101\n",
      "epoch 1866, loss 0.000101\n",
      "epoch 1867, loss 0.000101\n",
      "epoch 1868, loss 0.000101\n",
      "epoch 1869, loss 0.000101\n",
      "epoch 1870, loss 0.000101\n",
      "epoch 1871, loss 0.000101\n",
      "epoch 1872, loss 0.000101\n",
      "epoch 1873, loss 0.000101\n",
      "epoch 1874, loss 0.000101\n",
      "epoch 1875, loss 0.000101\n",
      "epoch 1876, loss 0.000101\n",
      "epoch 1877, loss 0.000101\n",
      "epoch 1878, loss 0.000101\n",
      "epoch 1879, loss 0.000101\n",
      "epoch 1880, loss 0.000101\n",
      "epoch 1881, loss 0.000101\n",
      "epoch 1882, loss 0.000101\n",
      "epoch 1883, loss 0.000101\n",
      "epoch 1884, loss 0.000101\n",
      "epoch 1885, loss 0.000101\n",
      "epoch 1886, loss 0.000101\n",
      "epoch 1887, loss 0.000101\n",
      "epoch 1888, loss 0.000101\n",
      "epoch 1889, loss 0.000101\n",
      "epoch 1890, loss 0.000101\n",
      "epoch 1891, loss 0.000101\n",
      "epoch 1892, loss 0.000101\n",
      "epoch 1893, loss 0.000101\n",
      "epoch 1894, loss 0.000101\n",
      "epoch 1895, loss 0.000101\n",
      "epoch 1896, loss 0.000101\n",
      "epoch 1897, loss 0.000101\n",
      "epoch 1898, loss 0.000101\n",
      "epoch 1899, loss 0.000101\n",
      "epoch 1900, loss 0.000101\n",
      "epoch 1901, loss 0.000101\n",
      "epoch 1902, loss 0.000101\n",
      "epoch 1903, loss 0.000101\n",
      "epoch 1904, loss 0.000101\n",
      "epoch 1905, loss 0.000101\n",
      "epoch 1906, loss 0.000101\n",
      "epoch 1907, loss 0.000101\n",
      "epoch 1908, loss 0.000101\n",
      "epoch 1909, loss 0.000101\n",
      "epoch 1910, loss 0.000101\n",
      "epoch 1911, loss 0.000101\n",
      "epoch 1912, loss 0.000101\n",
      "epoch 1913, loss 0.000101\n",
      "epoch 1914, loss 0.000101\n",
      "epoch 1915, loss 0.000101\n",
      "epoch 1916, loss 0.000101\n",
      "epoch 1917, loss 0.000101\n",
      "epoch 1918, loss 0.000101\n",
      "epoch 1919, loss 0.000101\n",
      "epoch 1920, loss 0.000101\n",
      "epoch 1921, loss 0.000101\n",
      "epoch 1922, loss 0.000101\n",
      "epoch 1923, loss 0.000101\n",
      "epoch 1924, loss 0.000101\n",
      "epoch 1925, loss 0.000101\n",
      "epoch 1926, loss 0.000101\n",
      "epoch 1927, loss 0.000101\n",
      "epoch 1928, loss 0.000101\n",
      "epoch 1929, loss 0.000101\n",
      "epoch 1930, loss 0.000101\n",
      "epoch 1931, loss 0.000101\n",
      "epoch 1932, loss 0.000101\n",
      "epoch 1933, loss 0.000101\n",
      "epoch 1934, loss 0.000101\n",
      "epoch 1935, loss 0.000101\n",
      "epoch 1936, loss 0.000101\n",
      "epoch 1937, loss 0.000101\n",
      "epoch 1938, loss 0.000101\n",
      "epoch 1939, loss 0.000101\n",
      "epoch 1940, loss 0.000101\n",
      "epoch 1941, loss 0.000101\n",
      "epoch 1942, loss 0.000101\n",
      "epoch 1943, loss 0.000101\n",
      "epoch 1944, loss 0.000101\n",
      "epoch 1945, loss 0.000101\n",
      "epoch 1946, loss 0.000101\n",
      "epoch 1947, loss 0.000101\n",
      "epoch 1948, loss 0.000101\n",
      "epoch 1949, loss 0.000101\n",
      "epoch 1950, loss 0.000101\n",
      "epoch 1951, loss 0.000101\n",
      "epoch 1952, loss 0.000101\n",
      "epoch 1953, loss 0.000101\n",
      "epoch 1954, loss 0.000101\n",
      "epoch 1955, loss 0.000101\n",
      "epoch 1956, loss 0.000101\n",
      "epoch 1957, loss 0.000101\n",
      "epoch 1958, loss 0.000101\n",
      "epoch 1959, loss 0.000101\n",
      "epoch 1960, loss 0.000101\n",
      "epoch 1961, loss 0.000101\n",
      "epoch 1962, loss 0.000101\n",
      "epoch 1963, loss 0.000101\n",
      "epoch 1964, loss 0.000101\n",
      "epoch 1965, loss 0.000101\n",
      "epoch 1966, loss 0.000101\n",
      "epoch 1967, loss 0.000101\n",
      "epoch 1968, loss 0.000101\n",
      "epoch 1969, loss 0.000101\n",
      "epoch 1970, loss 0.000101\n",
      "epoch 1971, loss 0.000101\n",
      "epoch 1972, loss 0.000101\n",
      "epoch 1973, loss 0.000101\n",
      "epoch 1974, loss 0.000101\n",
      "epoch 1975, loss 0.000101\n",
      "epoch 1976, loss 0.000101\n",
      "epoch 1977, loss 0.000101\n",
      "epoch 1978, loss 0.000101\n",
      "epoch 1979, loss 0.000101\n",
      "epoch 1980, loss 0.000101\n",
      "epoch 1981, loss 0.000101\n",
      "epoch 1982, loss 0.000101\n",
      "epoch 1983, loss 0.000101\n",
      "epoch 1984, loss 0.000101\n",
      "epoch 1985, loss 0.000101\n",
      "epoch 1986, loss 0.000101\n",
      "epoch 1987, loss 0.000101\n",
      "epoch 1988, loss 0.000101\n",
      "epoch 1989, loss 0.000101\n",
      "epoch 1990, loss 0.000101\n",
      "epoch 1991, loss 0.000101\n",
      "epoch 1992, loss 0.000101\n",
      "epoch 1993, loss 0.000101\n",
      "epoch 1994, loss 0.000101\n",
      "epoch 1995, loss 0.000101\n",
      "epoch 1996, loss 0.000101\n",
      "epoch 1997, loss 0.000101\n",
      "epoch 1998, loss 0.000101\n",
      "epoch 1999, loss 0.000101\n",
      "epoch 2000, loss 0.000101\n"
     ]
    }
   ],
   "source": [
    "# Run the training method\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimiser.zero_grad()\n",
    "        l.backward()\n",
    "        optimiser.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f\"epoch {epoch+1}, loss {l:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True weights (tensor([[ 2.0000],\n        [-3.3000]])), Predicted weights (tensor([[ 2.0001, -3.3003]])), error in estimating w: tensor([[-8.9884e-05],\n        [ 2.6298e-04]])\nTrue bais (tensor([4.5000])), Predicted bais (tensor([4.5005])), error in estimating b: tensor([-0.0005])\n"
     ]
    }
   ],
   "source": [
    "new_w = net.weight.data\n",
    "new_b = net.bias.data\n",
    "print(\"True weights ({}), Predicted weights ({}), error in estimating w: {}\".format(true_w, new_w, true_w - new_w.reshape(true_w.shape)))\n",
    "print(\"True bais ({}), Predicted bais ({}), error in estimating b: {}\".format(true_b, new_b, true_b - new_b.reshape(true_b.shape)))"
   ]
  }
 ]
}